{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68059c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Instantiate stopWords\n",
    "stopWords = stopwords.words(\"english\")\n",
    "    \n",
    "# Instantiate wordnet lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d5aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desc: Spelling correction and lemmatization\n",
    "Input: text (string) - text to be corrected and lemmatized\n",
    "Output: text (string) - corrected and lemmatized text\n",
    "\"\"\"\n",
    "def prepareText(text):\n",
    "    \n",
    "    # Spell Correction\n",
    "    text = ''.join(TextBlob(text).correct())\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    text = ' '.join([wn.lemmatize(word) for word in text.split(' ')])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34015c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desc: POS (Parts of Speech) tagging for Nouns, Pronouns, Verbs, Adjectives, and Adverbs\n",
    "Input: text (string)\n",
    "Output: 4 columns of their respective POS counts \n",
    "\"\"\"\n",
    "def posCount(text):\n",
    "    \n",
    "    # Tokenize the words in the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Assign POS tags to each words\n",
    "    pos = nltk.pos_tag(tokens, tagset='universal')\n",
    "    \n",
    "    # Count the POS tags\n",
    "    counts = Counter(tag for _, tag in pos)\n",
    "    \n",
    "    # Get individual counts for POS of interests\n",
    "    noun = counts[\"NOUN\"]\n",
    "    pronoun = counts[\"PRON\"]\n",
    "    verb = counts[\"VERB\"]\n",
    "    adjective = counts[\"ADJ\"]\n",
    "    adverb = counts[\"ADV\"]\n",
    "    \n",
    "    return noun, pronoun, verb, adjective, adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707fc009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>post_length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>DirtJunkie133</td>\n",
       "      <td>abd11x</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently diagnosed, need to talk to others who...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3021</td>\n",
       "      <td>recently diagnosed need talk others diagnosed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>Lin_the_pillow_artis</td>\n",
       "      <td>abd7q9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Really annoyed at my familys drunk friends So ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>751</td>\n",
       "      <td>really annoyed familys drunk friends family ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>Fleetfeathers</td>\n",
       "      <td>abda0t</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>The medication journey: a current disappointme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1713</td>\n",
       "      <td>medication journey current disappointment wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>UnleashedDebs</td>\n",
       "      <td>abdd13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wearables, REM sleep detected while gaming not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>988</td>\n",
       "      <td>wearables rem sleep detected gaming sleeping h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>liluglee</td>\n",
       "      <td>abdj4w</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Picking a friend up to carpool to a NYE party....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123</td>\n",
       "      <td>picking friend carpool nye party texted way dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652198</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>RussianPower69</td>\n",
       "      <td>ko0jbp</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Me sad Me has no one to talk to on New Year</td>\n",
       "      <td>Depression/Sadness</td>\n",
       "      <td>43</td>\n",
       "      <td>sad one talk new year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652199</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>prettygirlolivia</td>\n",
       "      <td>ko0lec</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Worst New Years Eve Ever My depression has hit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>318</td>\n",
       "      <td>worst new years eve ever depression hit bad la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652200</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>DirtyLizard0032</td>\n",
       "      <td>ko0rtl</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Check out my sad song</td>\n",
       "      <td>Depression/Sadness</td>\n",
       "      <td>22</td>\n",
       "      <td>check sad song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652201</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>Music-SunsetGirl490</td>\n",
       "      <td>ko0skv</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Zoom Wedding Tomorrow! So here is the story. L...</td>\n",
       "      <td>Loneliness</td>\n",
       "      <td>486</td>\n",
       "      <td>zoom wedding tomorrow story lets call groom bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652202</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>Monte_1997</td>\n",
       "      <td>ko0vrb</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>She kissed me and I cant remember She kissed m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271</td>\n",
       "      <td>kissed cant remember kissed silvester felt so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>652203 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit        date                author      id  num_comments  \\\n",
       "0           ADHD  01/01/2019         DirtJunkie133  abd11x          13.0   \n",
       "1           ADHD  01/01/2019  Lin_the_pillow_artis  abd7q9           5.0   \n",
       "2           ADHD  01/01/2019         Fleetfeathers  abda0t          12.0   \n",
       "3           ADHD  01/01/2019         UnleashedDebs  abdd13           4.0   \n",
       "4           ADHD  01/01/2019              liluglee  abdj4w           1.0   \n",
       "...          ...         ...                   ...     ...           ...   \n",
       "652198       sad  31/12/2020        RussianPower69  ko0jbp           3.0   \n",
       "652199       sad  31/12/2020      prettygirlolivia  ko0lec           8.0   \n",
       "652200       sad  31/12/2020       DirtyLizard0032  ko0rtl           2.0   \n",
       "652201       sad  31/12/2020   Music-SunsetGirl490  ko0skv           3.0   \n",
       "652202       sad  31/12/2020            Monte_1997  ko0vrb           2.0   \n",
       "\n",
       "        score                                               text  \\\n",
       "0           1  Recently diagnosed, need to talk to others who...   \n",
       "1           1  Really annoyed at my familys drunk friends So ...   \n",
       "2           1  The medication journey: a current disappointme...   \n",
       "3           1  Wearables, REM sleep detected while gaming not...   \n",
       "4           1  Picking a friend up to carpool to a NYE party....   \n",
       "...       ...                                                ...   \n",
       "652198      1        Me sad Me has no one to talk to on New Year   \n",
       "652199      1  Worst New Years Eve Ever My depression has hit...   \n",
       "652200      1                             Check out my sad song    \n",
       "652201      1  Zoom Wedding Tomorrow! So here is the story. L...   \n",
       "652202      1  She kissed me and I cant remember She kissed m...   \n",
       "\n",
       "           link_flair_text  post_length  \\\n",
       "0                      NaN         3021   \n",
       "1                      NaN          751   \n",
       "2                      NaN         1713   \n",
       "3                      NaN          988   \n",
       "4                      NaN          123   \n",
       "...                    ...          ...   \n",
       "652198  Depression/Sadness           43   \n",
       "652199                 NaN          318   \n",
       "652200  Depression/Sadness           22   \n",
       "652201          Loneliness          486   \n",
       "652202                 NaN          271   \n",
       "\n",
       "                                               clean_text  \n",
       "0       recently diagnosed need talk others diagnosed ...  \n",
       "1       really annoyed familys drunk friends family ho...  \n",
       "2        medication journey current disappointment wan...  \n",
       "3       wearables rem sleep detected gaming sleeping h...  \n",
       "4       picking friend carpool nye party texted way dr...  \n",
       "...                                                   ...  \n",
       "652198                              sad one talk new year  \n",
       "652199  worst new years eve ever depression hit bad la...  \n",
       "652200                                    check sad song   \n",
       "652201  zoom wedding tomorrow story lets call groom bo...  \n",
       "652202   kissed cant remember kissed silvester felt so...  \n",
       "\n",
       "[652203 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"./preproc_data/clean_text_all_subreddits.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with clean text\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(prepareText)\n",
    "\n",
    "# Drop any rows with NaN in clean_text\n",
    "df = df.dropna(subset=['clean_text'])\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove duplicate words\n",
    "df[\"unique_clean_text\"] = df[\"clean_text\"].apply(lambda x: \" \".join(dict.fromkeys(x.split())))\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "704bd7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 610 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1   2   3   4\n",
       "0   116  0  75  45  26\n",
       "1    31  0  16  10   7\n",
       "2    67  0  38  30  10\n",
       "3    32  0  25  19   9\n",
       "4     8  0   3   1   0\n",
       "..  ... ..  ..  ..  ..\n",
       "95   45  0  24  23   8\n",
       "96   30  0  26  16   7\n",
       "97   74  0  44  42  13\n",
       "98   12  0   6  15   2\n",
       "99   18  0  14   6   3\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get 5 most common POS counts\n",
    "test = df[:100].apply(lambda x: posCount(x['clean_text']), axis=1, result_type = \"expand\")\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2020d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 5 most common POS counts\n",
    "df[[\"noun\", \"pronoun\", \"verb\", \"adjective\", \"adverb\"]] = df.apply(lambda x: posCount(x['unique_clean_text']), axis=1, result_type = \"expand\")\n",
    "\n",
    "# Get word count of clean text\n",
    "df[\"word_count\"] = df[\"unique_clean_text\"].str.split().str.len()\n",
    "\n",
    "# Normalize POS Counts\n",
    "df[\"norm_noun\"] = df[\"noun\"] / df[\"word_count\"]\n",
    "df[\"norm_pronoun\"] = df[\"pronoun\"] / df[\"word_count\"]\n",
    "df[\"norm_verb\"] = df[\"verb\"] / df[\"word_count\"]\n",
    "df[\"norm_adj\"] = df[\"adj\"] / df[\"word_count\"]\n",
    "df[\"norm_adv\"] = df[\"norm_adv\"] / df[\"word_count\"]\n",
    "\n",
    "# Drop text column and re-arrange columns\n",
    "df = df[[\"subreddit\", \"date\", \"author\", \"id\", \"num_comments\", \"score\", \n",
    "         \"clean_text\", \"unique_clean_text\", \"post_length\", \"word_count\", \n",
    "         \"norm_noun\", \"norm_pronoun\", \"norm_verb\", \"norm_adj\", \"norm_adv\", \n",
    "         \"link_flair_text\"]]\n",
    "\n",
    "# Display\n",
    "display(df)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique link_flair_text\n",
    "for flair in df[\"link_flair_text\"].unique():\n",
    "    print(flair)\n",
    "\n",
    "print(len(df[\"link_flair_text\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efdec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis\n",
    "va = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create new column with compound sentiment\n",
    "df[\"compound_sent\"] = df[\"clean_text\"].apply(lambda x: va.polarity_scores(x)['compound'])\n",
    "\n",
    "# Display\n",
    "print(df)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cf002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TF-IDF Count Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "print(X.shape)\n",
    "print(df.shape)\n",
    "\n",
    "# Convert X to a df\n",
    "wc = pd.DataFrame.sparse.from_spmatrix(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create a row of sums\n",
    "wc.loc['Total'] = wc.sum()\n",
    "\n",
    "# Check\n",
    "display(wc.tail())\n",
    "\n",
    "# Sort wc by values in Total row\n",
    "wc.sort_values(by=\"Total\", axis=1, ascending=False, inplace=True)\n",
    "\n",
    "# Drop Total row and select first 2000 columns\n",
    "wc = wc.iloc[:-1, :2000]\n",
    "\n",
    "# Check\n",
    "display(wc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate wc with df\n",
    "df = pd.concat([df, wc], axis=1)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e223fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
