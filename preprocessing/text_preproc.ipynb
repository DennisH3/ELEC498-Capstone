{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a720d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\denni\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\denni\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\denni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\denni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\denni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\denni\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68059c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\denni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Instantiate stopWords\n",
    "stopWords = stopwords.words(\"english\")\n",
    "    \n",
    "# Instantiate wordnet lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d5aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desc: Spelling correction and lemmatization\n",
    "Input: text (string) - text to be corrected and lemmatized\n",
    "Output: text (string) - corrected and lemmatized text\n",
    "\"\"\"\n",
    "def prepareText(text):\n",
    "    \n",
    "    # Spell Correction\n",
    "    text = ''.join(TextBlob(text).correct())\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    text = ' '.join([wn.lemmatize(word) for word in text.split(' ')])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2326c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desc: POS (Parts of Speech) tagging for Nouns, Pronouns, Verbs, Adjectives, and Adverbs\n",
    "Input: text (string)\n",
    "Output: 4 columns of their respective POS counts \n",
    "\"\"\"\n",
    "def posCount(text):\n",
    "    \n",
    "    # Tokenize the words in the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Assign POS tags to each words\n",
    "    pos = nltk.pos_tag(tokens, tagset='universal')\n",
    "    \n",
    "    # Count the POS tags\n",
    "    counts = Counter(tag for _, tag in pos)\n",
    "    \n",
    "    # Get individual counts for POS of interests\n",
    "    noun = counts[\"NOUN\"]\n",
    "    pronoun = counts[\"PRON\"]\n",
    "    verb = counts[\"VERB\"]\n",
    "    adjective = counts[\"ADJ\"]\n",
    "    adverb = counts[\"ADV\"]\n",
    "    \n",
    "    return noun, pronoun, verb, adjective, adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d45bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./clean_data/clean_ADHD_2019_submission_data.csv', './clean_data/clean_ADHD_2020_submission_data.csv', './clean_data/clean_Anxiety_2019_submission_data.csv', './clean_data/clean_Anxiety_2020_submission_data.csv', './clean_data/clean_CasualConversation_2020_submission_data.csv', './clean_data/clean_CasualConversation_2021_submission_data.csv', './clean_data/clean_depression_help_2020_submission_data.csv', './clean_data/clean_happy_2017_submission_data.csv', './clean_data/clean_happy_2018_submission_data.csv', './clean_data/clean_happy_2019_submission_data.csv', './clean_data/clean_happy_2020_submission_data.csv', './clean_data/clean_happy_2021_submission_data.csv', './clean_data/clean_mentalhealth_L3YR_submission_data.csv', './clean_data/clean_overcoming_2020_submission_data.csv', './clean_data/clean_sad_2019_submission_data.csv', './clean_data/clean_sad_2020_submission_data.csv']\n"
     ]
    }
   ],
   "source": [
    "# If the preproc_data forlder does not exist, create it\n",
    "if not os.path.exists(\"./preproc_data/\"):\n",
    "    os.mkdir(\"./preproc_data/\")\n",
    "\n",
    "# Read the CSV files into a list\n",
    "try:\n",
    "    # List of csv files\n",
    "    csvs = [f.name for f in os.scandir(\"./clean_data/\") if f.name.endswith(\".csv\")]\n",
    "    \n",
    "    # Remove hidden directories\n",
    "    csvs = [f for f in csvs if not f.startswith('.')]\n",
    "    \n",
    "    # Append directory as prefix to strings in list\n",
    "    csvs = ['./clean_data/' + f for f in csvs]\n",
    "    \n",
    "    print(csvs)\n",
    "except:\n",
    "    print(\"The clean_data folder does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707fc009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./clean_data/clean_ADHD_2019_submission_data.csv\n",
      "./clean_data/clean_ADHD_2020_submission_data.csv\n",
      "./clean_data/clean_Anxiety_2019_submission_data.csv\n",
      "./clean_data/clean_Anxiety_2020_submission_data.csv\n",
      "./clean_data/clean_CasualConversation_2020_submission_data.csv\n",
      "./clean_data/clean_CasualConversation_2021_submission_data.csv\n",
      "./clean_data/clean_depression_help_2020_submission_data.csv\n",
      "./clean_data/clean_happy_2017_submission_data.csv\n",
      "./clean_data/clean_happy_2018_submission_data.csv\n",
      "./clean_data/clean_happy_2019_submission_data.csv\n",
      "./clean_data/clean_happy_2020_submission_data.csv\n",
      "./clean_data/clean_happy_2021_submission_data.csv\n",
      "./clean_data/clean_mentalhealth_L3YR_submission_data.csv\n",
      "./clean_data/clean_overcoming_2020_submission_data.csv\n",
      "./clean_data/clean_sad_2019_submission_data.csv\n",
      "./clean_data/clean_sad_2020_submission_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>link_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>DirtJunkie133</td>\n",
       "      <td>abd11x</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently diagnosed, need to talk to others who...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>Lin_the_pillow_artis</td>\n",
       "      <td>abd7q9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Really annoyed at my familys drunk friends So ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>Fleetfeathers</td>\n",
       "      <td>abda0t</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>The medication journey: a current disappointme...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>UnleashedDebs</td>\n",
       "      <td>abdd13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wearables, REM sleep detected while gaming not...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>liluglee</td>\n",
       "      <td>abdj4w</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Picking a friend up to carpool to a NYE party....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652198</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>RussianPower69</td>\n",
       "      <td>ko0jbp</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Me sad Me has no one to talk to on New Year</td>\n",
       "      <td>Depression/Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652199</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>prettygirlolivia</td>\n",
       "      <td>ko0lec</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Worst New Years Eve Ever My depression has hit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652200</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>DirtyLizard0032</td>\n",
       "      <td>ko0rtl</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Check out my sad song</td>\n",
       "      <td>Depression/Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652201</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>Music-SunsetGirl490</td>\n",
       "      <td>ko0skv</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Zoom Wedding Tomorrow! So here is the story. L...</td>\n",
       "      <td>Loneliness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652202</th>\n",
       "      <td>sad</td>\n",
       "      <td>31/12/2020</td>\n",
       "      <td>Monte_1997</td>\n",
       "      <td>ko0vrb</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>She kissed me and I cant remember She kissed m...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>652203 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit        date                author      id  num_comments  \\\n",
       "0           ADHD  01/01/2019         DirtJunkie133  abd11x          13.0   \n",
       "1           ADHD  01/01/2019  Lin_the_pillow_artis  abd7q9           5.0   \n",
       "2           ADHD  01/01/2019         Fleetfeathers  abda0t          12.0   \n",
       "3           ADHD  01/01/2019         UnleashedDebs  abdd13           4.0   \n",
       "4           ADHD  01/01/2019              liluglee  abdj4w           1.0   \n",
       "...          ...         ...                   ...     ...           ...   \n",
       "652198       sad  31/12/2020        RussianPower69  ko0jbp           3.0   \n",
       "652199       sad  31/12/2020      prettygirlolivia  ko0lec           8.0   \n",
       "652200       sad  31/12/2020       DirtyLizard0032  ko0rtl           2.0   \n",
       "652201       sad  31/12/2020   Music-SunsetGirl490  ko0skv           3.0   \n",
       "652202       sad  31/12/2020            Monte_1997  ko0vrb           2.0   \n",
       "\n",
       "        score                                               text  \\\n",
       "0           1  Recently diagnosed, need to talk to others who...   \n",
       "1           1  Really annoyed at my familys drunk friends So ...   \n",
       "2           1  The medication journey: a current disappointme...   \n",
       "3           1  Wearables, REM sleep detected while gaming not...   \n",
       "4           1  Picking a friend up to carpool to a NYE party....   \n",
       "...       ...                                                ...   \n",
       "652198      1        Me sad Me has no one to talk to on New Year   \n",
       "652199      1  Worst New Years Eve Ever My depression has hit...   \n",
       "652200      1                             Check out my sad song    \n",
       "652201      1  Zoom Wedding Tomorrow! So here is the story. L...   \n",
       "652202      1  She kissed me and I cant remember She kissed m...   \n",
       "\n",
       "           link_flair_text  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "652198  Depression/Sadness  \n",
       "652199                 NaN  \n",
       "652200  Depression/Sadness  \n",
       "652201          Loneliness  \n",
       "652202                 NaN  \n",
       "\n",
       "[652203 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty list to store dfs\n",
    "files = []\n",
    "\n",
    "# Apply basic preprocessing to each csv file\n",
    "for c in csvs:\n",
    "    print(c)\n",
    "    files.append(pd.read_csv(c))\n",
    "\n",
    "# Merge all files into one df\n",
    "df = pd.concat(files, ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc786d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>post_length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>DirtJunkie133</td>\n",
       "      <td>abd11x</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently diagnosed, need to talk to others who...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3021</td>\n",
       "      <td>recently diagnosed need talk others diagnosed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>Lin_the_pillow_artis</td>\n",
       "      <td>abd7q9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Really annoyed at my familys drunk friends So ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>751</td>\n",
       "      <td>really annoyed familys drunk friends family ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>Fleetfeathers</td>\n",
       "      <td>abda0t</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>The medication journey: a current disappointme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1713</td>\n",
       "      <td>medication journey current disappointment wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>UnleashedDebs</td>\n",
       "      <td>abdd13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wearables, REM sleep detected while gaming not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>988</td>\n",
       "      <td>wearables rem sleep detected gaming sleeping h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>liluglee</td>\n",
       "      <td>abdj4w</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Picking a friend up to carpool to a NYE party....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123</td>\n",
       "      <td>picking friend carpool nye party texted way dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit        date                author      id  num_comments  score  \\\n",
       "0      ADHD  01/01/2019         DirtJunkie133  abd11x          13.0      1   \n",
       "1      ADHD  01/01/2019  Lin_the_pillow_artis  abd7q9           5.0      1   \n",
       "2      ADHD  01/01/2019         Fleetfeathers  abda0t          12.0      1   \n",
       "3      ADHD  01/01/2019         UnleashedDebs  abdd13           4.0      1   \n",
       "4      ADHD  01/01/2019              liluglee  abdj4w           1.0      1   \n",
       "\n",
       "                                                text link_flair_text  \\\n",
       "0  Recently diagnosed, need to talk to others who...             NaN   \n",
       "1  Really annoyed at my familys drunk friends So ...             NaN   \n",
       "2  The medication journey: a current disappointme...             NaN   \n",
       "3  Wearables, REM sleep detected while gaming not...             NaN   \n",
       "4  Picking a friend up to carpool to a NYE party....             NaN   \n",
       "\n",
       "   post_length                                         clean_text  \n",
       "0         3021  recently diagnosed need talk others diagnosed ...  \n",
       "1          751  really annoyed familys drunk friends family ho...  \n",
       "2         1713   medication journey current disappointment wan...  \n",
       "3          988  wearables rem sleep detected gaming sleeping h...  \n",
       "4          123  picking friend carpool nye party texted way dr...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column for character count\n",
    "df[\"post_length\"] = df['text'].str.len()\n",
    "\n",
    "# Text Preprocessing\n",
    "# Lower case\n",
    "df[\"clean_text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# Remove unicode characters (emojis, etc.)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.encode('ascii', 'ignore').str.decode('utf-8')\n",
    "\n",
    "# Remove urls\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'http*\\S+', ' ', regex=True)\n",
    "\n",
    "# Remove multi-character symbols (\\n, \\t, \\r)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'[\\n\\r\\t]', ' ', regex=True)\n",
    "\n",
    "# Remove numeric values\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'[0-9]', ' ', regex=True)\n",
    "\n",
    "# Reduce repeated letters\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(re.compile(r\"(.)\\1{2,}\"), r\"\\1\\1\", regex=True)\n",
    "\n",
    "# Remove stop words\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stopWords))\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(pat, '', regex=True)\n",
    "\n",
    "# Remove punctuation\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace('[%s]' % re.escape(string.punctuation), ' ', regex=True)\n",
    "\n",
    "# Remove stop words again (in case stop word was next to punctuation)\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stopWords))\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(pat, '', regex=True)\n",
    "\n",
    "# Remove extra blank spaces\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.replace(r'\\s{2,}', ' ', regex=True)\n",
    "\n",
    "# Check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a5b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell correction and lemmatize\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(prepareText)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with NaN in clean_text\n",
    "df = df.dropna(subset=['clean_text'])\n",
    "\n",
    "# Drop any rows with NaN in clean_text\n",
    "df = df.dropna(subset=['clean_text'])\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove duplicate words\n",
    "df[\"unique_clean_text\"] = df[\"clean_text\"].apply(lambda x: \" \".join(dict.fromkeys(x.split())))\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 5 most common POS counts\n",
    "df[[\"noun\", \"pronoun\", \"verb\", \"adjective\", \"adverb\"]] = df.apply(lambda x: posCount(x['unique_clean_text']), axis=1, result_type = \"expand\")\n",
    "\n",
    "# Get word count of clean text\n",
    "df[\"word_count\"] = df[\"unique_clean_text\"].str.split().str.len()\n",
    "\n",
    "# Normalize POS Counts\n",
    "df[\"norm_noun\"] = df[\"noun\"] / df[\"word_count\"]\n",
    "df[\"norm_pronoun\"] = df[\"pronoun\"] / df[\"word_count\"]\n",
    "df[\"norm_verb\"] = df[\"verb\"] / df[\"word_count\"]\n",
    "df[\"norm_adj\"] = df[\"adj\"] / df[\"word_count\"]\n",
    "df[\"norm_adv\"] = df[\"norm_adv\"] / df[\"word_count\"]\n",
    "\n",
    "# Drop text column and re-arrange columns\n",
    "df = df[[\"subreddit\", \"date\", \"author\", \"id\", \"num_comments\", \"score\", \n",
    "         \"clean_text\", \"unique_clean_text\", \"post_length\", \"word_count\", \n",
    "         \"norm_noun\", \"norm_pronoun\", \"norm_verb\", \"norm_adj\", \"norm_adv\", \n",
    "         \"link_flair_text\"]]\n",
    "\n",
    "# Display\n",
    "display(df)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique link_flair_text\n",
    "for flair in df[\"link_flair_text\"].unique():\n",
    "    print(flair)\n",
    "\n",
    "print(len(df[\"link_flair_text\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efdec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis\n",
    "va = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create new column with compound sentiment\n",
    "df[\"compound_sent\"] = df[\"clean_text\"].apply(lambda x: va.polarity_scores(x)['compound'])\n",
    "\n",
    "# Display\n",
    "print(df)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cf002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TF-IDF Count Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "print(X.shape)\n",
    "print(df.shape)\n",
    "\n",
    "# Convert X to a df\n",
    "wc = pd.DataFrame.sparse.from_spmatrix(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Create a row of sums\n",
    "wc.loc['Total'] = wc.sum()\n",
    "\n",
    "# Check\n",
    "display(wc.tail())\n",
    "\n",
    "# Sort wc by values in Total row\n",
    "wc.sort_values(by=\"Total\", axis=1, ascending=False, inplace=True)\n",
    "\n",
    "# Drop Total row and select first 2000 columns\n",
    "wc = wc.iloc[:-1, :2000]\n",
    "\n",
    "# Check\n",
    "display(wc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate wc with df\n",
    "df = pd.concat([df, wc], axis=1)\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"./preproc_data/clean_text_all_subreddits_BoW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607aadd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
